{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "As mentioned in the Udacity lecture video, Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.\n",
    "\n",
    "#### Algorithm Architecture\n",
    "\n",
    "Here's the framework of the code:\n",
    "1. Prepare the Unity environment and Import the necessary packages\n",
    "2. Check the Unity environment\n",
    "3. Define functions to instanciate and train a DDPG agent\n",
    "4. Train an agent using DDPG\n",
    "5. Present results\n",
    "\n",
    "\n",
    "The algo contains actor and critic pair of neural networks. The actor NN uses the following flow:\n",
    "0. Input nodes = 33 (length of available states)\n",
    "1. Fully Connected Layer (128 nodes, Relu activation) \n",
    "2. Batch Normlization\n",
    "3. Fully Connected Layer (128 nodes, Relu activation) \n",
    "4. Ouput nodes (4 nodes, which is the length of available actions, tanh activation)\n",
    "\n",
    "The Critic NN uses the following flow:\n",
    "0Input nodes = 33 (length of available states)\n",
    "1. Fully Connected Layer (128 nodes, Relu activation) \n",
    "2. Batch Normlization\n",
    "3. Include Actions at the second fully connected layer\n",
    "4. Fully Connected Layer (128+4 nodes, Relu activation) \n",
    "5. Ouput node (1 node, no activation)\n",
    "\n",
    "\n",
    "##### Environment and DDPG Parameters\n",
    "\n",
    "- state_size. This is the environment state size. \n",
    "- action_size. This is the different actions that the agent can take.\n",
    "- buffer_size. This is how much the cache can store regarding past experiences\n",
    "- batch_size. In the buffer, this is how many samples are being taken out at one time\n",
    "\n",
    "#### Hyperparameters\n",
    "##### Model Related\n",
    "\n",
    "- gamma. Reward discount factor\n",
    "- tau. Soft Update: weight_target = tau *weight_local + (1 - tau) * weight_target\n",
    "- lr_actor. learning rate of the actor NN\n",
    "- lr_critic. learning rate of the critic NN\n",
    "- actor_fc1_units. Number of units for the layer 1 in the actor model\n",
    "- actor_fc1_units. Number of units for the layer 2 in the actor model\n",
    "- critic_fcs1_units. Number of units for the layer 1 in the critic model\n",
    "- critic_fc2_units. Number of units for the layer 2 in the critic model\n",
    "\n",
    "\n",
    "##### Noise Related\n",
    "\n",
    "- add_ounoise. Add Ornstein-Uhlenbeck noise or not?\n",
    "- mu. Ornstein-Uhlenbeck noise parameter\n",
    "- theta. Ornstein-Uhlenbeck noise parameter\n",
    "- sigma. Ornstein-Uhlenbeck noise parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "The libraries that are required to run the code are the following:\n",
    "\n",
    "- unityagents\n",
    "- torch\n",
    "- numpy\n",
    "- random\n",
    "- copy\n",
    "- time\n",
    "- collections\n",
    "- matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment solved in 217 episodes with an Average Score of 30.00... If we let more episodes run, the average scores will get higher.\n",
    "\n",
    "Here's the results from the DDPG algo:\n",
    "\n",
    "<img style=\"left;\" src=\"ddpg_performance_20191105.png\" width=\"500\">\n",
    "\n",
    "Also note that the rate of learning improvement increased at around 100 episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is natual to go on to the second portion of the exercise, which is to train the 20 agents. I would be intereted to see how DDPG, or different algos, such as PPO and A3C would perform. Would it converge faster, less noisy? How are the biases and variances for each of these different algos? Which algo take longer to run? These will will help me build more intuition on what algos to choose base on their pro and cons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
